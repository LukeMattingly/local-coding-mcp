to run ollama with existing local model
`ollama run hf.co/unsloth/Devstral-Small-2505-GGUF:Q4_K_M`

to run ui
`open-webui serve`

then is available via 
`http://localhost:8080/`

